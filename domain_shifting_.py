# -*- coding: utf-8 -*-
"""domain_shifting (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g5gBessr-scoLEJgDFLwBzvEa5DooGun

## Distribution Shift Analysis
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import numpy as np
import os
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score


# define path
y_train_path = 'y_train.csv'
y_test_path = 'y_test_2_reduced.csv'
x_train_path = 'X_train.csv'
x_test_one_path = 'X_test_1.csv'
x_test_two_path = 'X_test_2.csv'


# read df
y = pd.read_csv(y_train_path)
y_test = pd.read_csv(y_test_path)
X_train_df = pd.read_csv(x_train_path)
X_test_one = pd.read_csv(x_test_one_path)
X_test_two = pd.read_csv(x_test_two_path)

test_length = y_test.shape[0]
test_indices = np.arange(test_length)
X_test_two_labelled = X_test_two.iloc[test_indices]
mask = np.ones(len(X_test_two), dtype=bool)
mask[test_indices] = False
X_test_two_unlabelled = X_test_two.iloc[mask]
# print(X_test_two_labelled.shape)
# print(X_test_two_unlabelled.shape)
ce_score_results = {'base_model':[],
                    'prior_shift_adapted': [],
                    'covariate_shift_adapted': [],
                    'class_conditional_shift_adapted':[]
                   }


# Weighted CE score for evaluate the methods results
def weighted_log_loss(y_true_np, y_pred):
    """
    Compute the weighted cross-entropy (log loss) given true labels and predicted probabilities.

    Parameters:
    - y_true: 1-d array like object contains (range 0-27), shape: (n_samples,)
    - y_pred: array like object contains list of probabilities, shape: (n_samples, 28)

    Returns:
    - Weighted log loss (scalar).
    """
    import numpy as np
    import pandas as pd

    # Number of classes
    n_classes = 28  # Classes 0-27

    # Convert discrete labels to one-hot encoded format
    def to_one_hot(labels, num_classes):
        one_hot = np.zeros((len(labels), num_classes))
        for i, label in enumerate(labels):
            if 0 <= label < num_classes:  # Safety check
                one_hot[i, label] = 1
        return one_hot

    # Convert true labels to one-hot format
    y_true_one_hot = to_one_hot(y_true_np, n_classes)


    # Compute class frequencies
    class_counts = np.sum(y_true_one_hot, axis=0)  # Sum over samples to get counts per class

    # Compute class weights with safety check for zero counts
    class_weights = np.zeros_like(class_counts)
    for c in range(n_classes):
        if class_counts[c] > 0:  # Avoid division by zero
            class_weights[c] = 1.0 / class_counts[c]

    # Normalize weights to sum to 1
    class_weights /= np.sum(class_weights)

    # Compute weighted loss
    sample_weights = np.sum(y_true_one_hot * class_weights, axis=1)  # Get weight for each sample

    # Calculate log loss term
    log_terms = np.sum(y_true_one_hot * np.log(y_pred), axis=1)
    loss = -np.mean(sample_weights * log_terms)

    return loss



def evaluate_model(model, X, y_true):
    """
        model: fitted model
        X: feature matrix (n_sample, 300)
        y_true: array, contains true labels (0-27), (n_samples, )
    """
    y_pred_prob = model.predict_proba(X)  # (n_samples, 28)
    y_pred_label = model.predict(X)
    ce = weighted_log_loss(y_true, y_pred_prob)
    print(f"Weighted CE: {ce:.4f}")
    print(classification_report(y_true, y_pred_label))
    return ce

"""## Prior Shift/Label Shift Analysis
**Definition**:  $P_{\text{train}}(y) \neq P_{\text{test}}(y)$ while  $P(X|Y)$ remains

### Method 1: Normalised Class Distribution Comparison
Check if the normalised class distribution differs between training and labelled test data.
"""

import seaborn as sns

plots_result_save_path = 'plots/results'
os.makedirs(plots_result_save_path, exist_ok = True)

# Compute class distributions
train_class_dist = y['label'].value_counts(normalize=True)
test_class_dist = y_test['label'].value_counts(normalize=True)
print(f"Example Change Class 5: {train_class_dist[5]*100:.2f} to {test_class_dist[5]*100:.2f}")

# Visual comparison
plt.figure()
sns.barplot(x=train_class_dist.index, y=train_class_dist, alpha=0.5, label='Train')
sns.barplot(x=test_class_dist.index, y=test_class_dist, alpha=0.5, label='Test')
plt.legend()
plt.title("Normalised Class Distribution Comparison")
plt.savefig(f'{plots_result_save_path}/class_disctribution_normalised.png')
plt.show()

"""### Method 2: Categorical Testing (Chi-Squared Test)
Reference: Rabanser, S., Günnemann, S., & Lipton, Z. C. (2019). Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift. Advances in Neural Information Processing Systems (NeurIPS 2019).

**Alpha** a significance level of $\alpha = 0.05$.
"""

from scipy.stats import chisquare

def detect_label_shift(y_train, y_test, alpha=0.05):
    observed = np.bincount(y_test)
    expected = np.bincount(y_train) * len(y_test) / len(y_train)
    stat, pval = chisquare(observed, expected)
    return pval < alpha

print(f"Label shift detected by chi-square test: {detect_label_shift(y.values.flatten(), y_test.values.flatten())}")

"""### Method 3: Distribution Testing (Jensen-Shannon divergence)
Use Jensen-Shannon divergence to detect label drifting instead of using the KL divergence. JS divergence is a more suitable measure of distance between two probability distributions than the KL divergence, since it has following properties: more symmetric $(JSD(X || Y) = JSD(Y || X))$, bounded, and robust in the presence of mixed shifts (covariates + concepts); KL divergence is only available when the distributions are strictly overlapping.
"""

from scipy.spatial.distance import jensenshannon
n_classes = y['label'].unique().shape[0]
train_dist = np.bincount(y['label'], minlength=n_classes+1) / len(y)
test_dist = np.bincount(y_test['label'], minlength=n_classes+1) / len(y_test['label'])
js_divergence = jensenshannon(train_dist, test_dist, base=2)
print(f"Jensen-Shannon Divergence between Train Set and Test Set 2(Labelled): {js_divergence:.2f}")

"""### Comprehensive Analysis of Label Shift Detection

---

 **1. Detection of Label Shift**  
Label shift occurs when $ P_{\text{train}}(y) \neq P_{\text{test}}(y)$, while $P(X|Y)$ remains unchanged. Three methods were used to detect this shift:

#### **a) Normalized Class Distribution Comparison**  
- **Key Observations**:  
  - Severe class proportion mismatches:  
    - One class dropped from **44.5%** (train) to **4.5%** (test).  
    - Some classes were entirely absent in the test set.  
  - **Conclusion**: Clear evidence of label shift due to inconsistent class prevalence.  

#### **b) Chi-Squared Test**  
- **Result**:  
  - Test statistic: Significant $p$-value $< 0.05$.  
  - Null hypothesis ($ H_0$: $ P_{\text{train}}(y) = P_{\text{test}}(y)$) **rejected**.  
- **Implication**: Statistical confirmation of label shift.  

#### **c) Jensen-Shannon Divergence (JSD)**  
- **Result**:  
  - JSD = **0.57** (range: 0-1).  
  - Interpretation: High divergence indicates **severe distribution mismatch**.  

---

### Prior adapatation Approach

#### Approach 1: Importance Weighting
Reweight training examples based on the ratio of test-to-train class frequencies.
"""

import numpy as np
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# define scaler
scaler = MinMaxScaler()
# scaler = StandardScaler()
scaler.fit(X_train_df)

# transform data
X_train_scaled = scaler.transform(X_train_df)
y_train = y.values.flatten()
X_val = scaler.transform(X_test_two_labelled)
X_test_two_whole = scaler.transform(X_test_two)
X_test_one_whole = scaler.transform(X_test_one)
X_test_two_unlabelled = scaler.transform(X_test_two_unlabelled)
y_val = y_test



# Compute class weights for adaptation using validation set priors
def compute_prior_weights(y_source, y_target):
    # Compute class weights for adaptation using validation set priors
    y_source = y_source
    y_target = y_target.values.flatten()
    classes = np.unique(y_source)
    source_counts = np.bincount(y_source)
    source_freq = source_counts / len(y_source)

    # Ensure target counts align with source classes
    target_counts = np.bincount(y_target, minlength=len(classes))
    target_freq = target_counts / len(y_target)

    # Calculate class weight ratios (target/source)
    class_weights = target_freq / source_freq
    class_weights_dict = dict(enumerate(class_weights))
    return class_weights_dict

class_weights_dict = compute_prior_weights(y_train, y_val)

# Train models
# Without adaptation (uses uniform class weights)
base_model = LogisticRegression(max_iter=1000, random_state=0)
base_model.fit(X_train_scaled, y_train)

# With prior adaptation (adjusted class weights)
model_adapted = LogisticRegression(max_iter=1000, random_state=0,
                                  class_weight=class_weights_dict)
model_adapted.fit(X_train_scaled, y_train)

# Evaluate on validation set
print("Without Adaptation:")
y_pred = base_model.predict_proba(X_val)
y_pred_label = base_model.predict(X_val)
no_adpt_ce = weighted_log_loss(y_val.values.flatten(), y_pred)
print(f"Weighted CE score: {no_adpt_ce}")
print(classification_report(y_val, y_pred_label))


print("\nWith Prior Adaptation:")
y_pred_adapted = model_adapted.predict_proba(X_val)
y_pred_adapted_label = model_adapted.predict(X_val)
adpt_ce = weighted_log_loss(y_val.values.flatten(), y_pred_adapted)
print(f"Weighted CE score: {adpt_ce}")
print(classification_report(y_val, y_pred_adapted_label))
print(f"Weighted CE Gained: {adpt_ce - no_adpt_ce:.4f}")

# add ce score
ce_score_results['base_model'].append(no_adpt_ce)
ce_score_results['prior_shift_adapted'].append(adpt_ce)

"""#### Remediation Strategies Result Discussion

 **Remediation Strategies**  
Importance Weighting methods were tested on a logistic regression model:

#### **Importance Weighting (IW)**  
- **Approach**: Reweight training samples by $\frac{P_{\text{test}}(y)}{P_{\text{train}}(y)}$.  
- **Result**:  
  - **Weighted CE loss reduced by 0.0049**.  
  - Improved alignment with test distribution.  
---

 **3. Discussion**  

#### **Why Importance Weighting Succeeded**  
1. **Theoretical Alignment**: Label shift assumes $ P(X|Y)$ is stable. IW directly corrects $P(y)$ (Shimodaira, 2000).  
2. **Simplicity**: Computationally lightweight and interpretable.  
3. **Class Ratio Focus**: Targets label proportions without overcomplicating feature space adjustments.  

#### **Practical Implications**  
- **IW Limitations**: Relies on accurate $P_{\text{test}}(y)$ estimation.  

**References**  
- Shimodaira, H. (2000). *Improving Predictive Inference Under Covariate Shift by Weighting the Log-Likelihood Function*.
- Rabanser, S., Günnemann, S., & Lipton, Z. C. (2019). Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift. Advances in Neural Information Processing Systems (NeurIPS 2019)
---

**Key Takeaway**: Importance weighting is a simple yet effective method for label shift correction.

## Covariate Shift
**Definition**: $P_{\text{train}}(X) \neq P_{\text{test}}(X)$ while  $P(Y|X)$ remains
Goal: Detect differences in feature distributions $P(X)$ between training and test sets, independent of labels.

### Method 1: Model-Based Detection (Domain Classifier)
**Auc threshold**: 0.6, Lipton et al. (2018) in their paper on detecting distribution shifts. The fundamental concept is that if two datasets come from the same distribution, a classifier should not be able to reliably distinguish between them (AUC ≈ 0.5). Conversely, if a classifier can distinguish them (AUC significantly above 0.5), the datasets likely come from different distributions.

Reference:
- Rabanser, S., Günnemann, S., & Lipton, Z. C. (2019). Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift. Advances in Neural Information Processing Systems (NeurIPS 2019).
-  Lipton, Z. C., Wang, Y. X., & Smola, A. (2018). Detecting and correcting for label shift with black box predictors. International Conference on Machine Learning.
"""

def domain_classifier(dataset1, dataset2, title=None, th=0.6):
    # Combine training and test data
    X_train_domain = np.vstack([dataset1, dataset2])
    y_train_domain = np.hstack([np.zeros(len(dataset1)), np.ones(len(dataset2))])

    # Train classifier
    X_train, X_val, y_train, y_val = train_test_split(X_train_domain, y_train_domain, test_size=0.2)
    clf = RandomForestClassifier().fit(X_train, y_train)
    auc = roc_auc_score(y_val, clf.predict_proba(X_val)[:, 1])
    print(f"AUC: {auc:.3f} between {title}")  # AUC > 0.7 → domain shift
    if auc > th:
        print(f"Covairate Shift Detected")

# test domain shift between the same dataset
# domain_classifier(X_train_df, X_train_df, 'Self comparison') # AUC: 0.021
domain_classifier(X_train_df, X_test_one, 'Training - Test One') # AUC: 0.483
domain_classifier(X_train_df, X_test_two, 'Training - Test Two(Whole)') # AUC: 0.725
domain_classifier(X_train_df, X_test_two_labelled, 'Training - Test Two(labelled)') # AUC: 0.690
domain_classifier(X_train_df, X_test_two_unlabelled, 'Training - Test Two(unlabelled)') # AUC: 0.716
domain_classifier(X_test_two_labelled, X_test_two_unlabelled, 'Test Two(labelled) - Test Two(unlabelled)') # AUC: 0.464

"""### Method 2:  PCA Reconstruction Loss
**Principal Components Analysis**

> We can detect covariate shift by comparing the PCA reconstruction error from production data to its expected level.
>
> Remaining question: what is the *expected reconstruction error*, and what does *significantly larger* mean?
> Split Train set into two parts, give the indicator of expected reconstruction error, then compare to test set 2.

PCA transformation is invertible, (compress feature by PCA and extract feature back) get the reconstruction error that occur during the process of lossy compression process

low reconstruction errors => indicate no covariate shift happens(if using the PCA mapping on train set into test set 2, get the reconstruction error, compare the compress and decompress)  

Using the approach from nannyml to detect covariant shift in multivariate feature drift, using the PCA reconstruction Error as the metric.

Reference:
https://www.nannyml.com/blog/detecting-covariate-shift-multivariate-approach
"""

# import libraries
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score

# NOTE: using standard normalization
scaler = StandardScaler()
X_train_df_scaled = pd.DataFrame(scaler.fit_transform(X_train_df))
X_test_one_scaled = pd.DataFrame(scaler.transform(X_test_one))
X_test_two_scaled = pd.DataFrame(scaler.transform(X_test_two))

# split X_train_df into two part to get standard PCA reconstructed data
X_train_1, X_train_2, y_train_1, y_train_2 = train_test_split(X_train_df_scaled, y, test_size=0.3, random_state=0, stratify=y)

import nannyml as nml
from sklearn.decomposition import IncrementalPCA

feature_column_names = X_train_df.columns.tolist()

n_components = 50
# sqrt(n_features)
# log2(n_features)

chunk_size = 500
# chunk_size='auto'

class HighDimReconstructor(nml.DataReconstructionDriftCalculator):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._reconstructor = IncrementalPCA(n_components=n_components)

# init nml drift calculator
drift_calculator = HighDimReconstructor(
    column_names=feature_column_names,
    chunk_size=chunk_size,
    n_components=n_components
).fit(reference_data=X_train_df)

# analysis test set
drift_result1 = drift_calculator.calculate(data=X_test_one)

drift_result2 = drift_calculator.calculate(data=X_test_two)

fig = drift_result1.plot(
    kind='drift',
    plot_reference=True,
    metric='reconstruction_error',
    confidence_band_alpha=0.2
)
fig.update_layout(title="Drift Analysis of Test Set 1")
fig.show()
fig.write_image("plots/results/drift_analysis_test_set1.png")

fig = drift_result2.plot(
    kind='drift',
    plot_reference=True,
    metric='reconstruction_error',
    confidence_band_alpha=0.2
)
fig.update_layout(title="Drift Analysis of Test Set 2")
fig.show()
fig.write_image("plots/results/drift_analysis_test_set2.png")


significant_chunks = drift_result2.filter(
    period='analysis',
    drifted=True
)
print(f"Number of significant shifting chunks: {len(significant_chunks)}")

# output significant shifting features
if hasattr(drift_calculator._reconstructor, 'components_'):
    components = drift_calculator._reconstructor.components_
    feature_importance = np.sum(np.abs(components), axis=0)
    top10_features_idx = np.argsort(feature_importance)[-10:]
    top10_features = [feature_column_names[i] for i in top10_features_idx]
    print("TOP 10 Significant Shifting Features:", top10_features)

"""### Approach 1: Kernel-based weight adapatation

**Disadvantage:**   
**Hyperparameter Sensitivity**: Requires precise tuning (bandwidth, kernel type) for high-dimensional data.  
**Overfitting Risk**: Small test sets or underrepresented classes amplify noise.  

Reference:
- Gretton, A., et al. (2009). *A Kernel Method for the Two-Sample Problem*.
- Huang, J., Smola, A., Gretton, A., Borgwardt, K., & Schölkopf, B. (2006). Correcting Sample Selection Bias by Unlabeled Data.
"""

import numpy as np
from sklearn.metrics import pairwise_kernels
from sklearn.linear_model import LogisticRegression

# ------------------------------------------
# Enhanced Weight Estimation with Gamma Tuning
# ------------------------------------------
def estimate_weights(X_source, X_target, gamma=0.0008):
    """
    Correctly implements Kernel Mean Matching (KMM) for covariate shift.
    Follows Huang et al.'s optimization-free approximation.
    """
    # Ensure input consistency
    n_source = X_source.shape[0]
    n_target = X_target.shape[0]

    # Tune gamma on TARGET distribution
    if gamma is None:
        gammas = np.logspace(-3, 2, 6)  # [0.001, 0.01, ..., 100]
        best_gamma = 1.0 / X_target.shape[1]
        best_mmd = np.inf
        # Find gamma that minimizes MMD between target and itself
        for g in gammas:
            K_target = pairwise_kernels(X_target, X_target, metric='rbf', gamma=g)
            mmd = np.abs(K_target.mean() - 2*K_target.mean() + K_target.mean())
            if mmd < best_mmd:
                best_gamma, best_mmd = g, mmd
        gamma = best_gamma

    # Compute source-target kernel matrix (shape: [n_source, n_target])
    K = pairwise_kernels(X_source, X_target, metric='rbf', gamma=gamma)

    # Compute weights as density ratio (key fix: aggregate over target samples)
    weights = K.mean(axis=1)  # Shape: (n_source,)

    # Normalize weights to prevent exploding gradients
    weights = weights / (weights.mean() + 1e-8)  # Mean ~1

    print(f"Gamma value: {gamma}")

    return weights

# - X_train_scaled, y_train: Training data
# - X_val, y_val: Labeled validation set (Test 2)
# - X_test: whole test 2 data

# Get validation-optimized weights

# weights  generated from trian and test set two labelled
weights = estimate_weights(X_train_scaled, X_val)

# weights  generated from trian and test set two whole
# weights = estimate_weights(X_train_scaled, X_test_two_whole)

# # weights  generated from trian and test set two unlabelled
# weights = estimate_weights(X_train_scaled, X_test_two_unlabelled)

# Train model with instance weights + class balancing
kernel_model = LogisticRegression(
    max_iter=1000,
    random_state=0,
    class_weight='balanced'
)
kernel_model.fit(X_train_scaled, y_train, sample_weight=np.clip(weights, 0.1, 10))



print("\nWith Covariate Adaptation(Kernel-based weight adaptation):")
print("=== Test Set 2 (Labelled Validation) ===")
kernel_adapted_ce = evaluate_model(kernel_model, X_val, y_val.values.flatten())

print(f"Weighted CE Gained: {kernel_adapted_ce - no_adpt_ce:.4f}")

# add ce score
ce_score_results['covariate_shift_adapted'].append(kernel_adapted_ce)

"""### Approach 2: CORAL
Concept: Align second-order statistics (covariance) between domains.
Why: Works well for linear shifts.
"""

def coral_adapt(X_source, X_target):
    # Covariance alignment
    cov_source = np.cov(X_source.T) + np.eye(X_source.shape[1])*1e-5
    cov_target = np.cov(X_target.T) + np.eye(X_target.shape[1])*1e-5
    adapt_operator = np.linalg.inv(cov_source) @ cov_target
    return X_source @ adapt_operator

X_train_adapted = coral_adapt(X_train_scaled, X_val)
coral_model = LogisticRegression(max_iter=1000,
    random_state=0,
    class_weight='balanced')
coral_model.fit(X_train_adapted, y_train)

print("\nWith Covariate Adaptation(Coral Feature Matrix Adaptation):")
print("=== Test Set 2 (Labelled Validation) ===")
coral_adapted_ce = evaluate_model(coral_model, X_val, y_val.values.flatten())

print(f"Weighted CE Gained: {coral_adapted_ce - no_adpt_ce:.4f}")

ce_score_results['covariate_shift_adapted'].append(coral_adapted_ce)

"""### Approach 3: Dynamic Distribution Adaptation (DDA)
Concept: Balance marginal (covariate) and conditional (label) distribution shifts.   

Goal: Addresses compound shifts where both $P(X)$ and $P(Y|X)$ change.
"""

from sklearn.metrics import pairwise_kernels


def compute_prior_weights(y_source, y_target):
    # Compute class weights for adaptation using validation set priors
    y_source = y_source.values.flatten()
    y_target = y_target.values.flatten()
    classes = np.unique(y_source)
    source_counts = np.bincount(y_source)
    source_freq = source_counts / len(y_source)

    # Ensure target counts align with source classes
    target_counts = np.bincount(y_target, minlength=len(classes))
    target_freq = target_counts / len(y_target)

    # Calculate class weight ratios (target/source)
    class_weights = target_freq / source_freq
    class_weights_dict = dict(enumerate(class_weights))
    return class_weights_dict

def compute_marginal_weights(X_source, X_target, gamma=0.0008):
    """
    Correctly implements Kernel Mean Matching (KMM) for covariate shift.
    Follows Huang et al.'s optimization-free approximation.
    """
    # Ensure input consistency
    n_source = X_source.shape[0]
    n_target = X_target.shape[0]

    # Tune gamma on TARGET distribution
    if gamma is None:
        gammas = np.logspace(-3, 2, 6)  # [0.001, 0.01, ..., 100]
        best_gamma = 1.0 / X_target.shape[1]
        best_mmd = np.inf
        # Find gamma that minimizes MMD between target and itself
        for g in gammas:
            K_target = pairwise_kernels(X_target, X_target, metric='rbf', gamma=g)
            mmd = np.abs(K_target.mean() - 2*K_target.mean() + K_target.mean())
            if mmd < best_mmd:
                best_gamma, best_mmd = g, mmd
        gamma = best_gamma

    # Compute source-target kernel matrix (shape: [n_source, n_target])
    K = pairwise_kernels(X_source, X_target, metric='rbf', gamma=gamma)

    # Compute weights as density ratio (key fix: aggregate over target samples)
    weights = K.mean(axis=1)  # Shape: (n_source,)

    # Normalize weights to prevent exploding gradients
    weights = weights / (weights.mean() + 1e-8)  # Mean ~1

    print(f"Gamma value: {gamma}")

    return weights


def compute_conditional_weights(y_source, y_target):
    # Estimate class probabilities
    classes, p_source = np.unique(y_source, return_counts=True)
    _, p_target = np.unique(y_target, return_counts=True)
    p_source = p_source / len(y_source)
    p_target = p_target / len(y_target)

    # Handle missing classes in target (add smoothing)
    epsilon = 1e-8
    p_target = np.array([p_target[c] if c < len(p_target) else epsilon for c in classes])
    p_source = np.array([p_source[c] for c in classes])

    # Compute weights for each sample
    class_ratios = p_target / (p_source + epsilon)
    return np.array([class_ratios[y] for y in y_source])  # Shape: (n_source,)

# Compute Dynamic Coefficient
def compute_alpha(X_source, X_target, y_source, y_target, gamma=0.001):
    """
    Computes the dynamic alpha for reweighting samples based on Marginal and Conditional MMD.

    Args:
        X_source (np.ndarray): Source domain features, shape (n_source_samples, n_features).
        X_target (np.ndarray): Target domain features, shape (n_target_samples, n_features).
        y_source (np.ndarray): Source domain labels, shape (n_source_samples,).
        y_target (np.ndarray): Target domain labels, shape (n_target_samples,).
        gamma (float, optional): RBF kernel parameter. Defaults to 0.1.

    Returns:
        float: The computed alpha value, clipped to [0.2, 0.8].
    """
    # Marginal MMD (features)
    K_marginal = pairwise_kernels(X_source, X_target, metric='rbf', gamma=gamma)
    mmd_marginal = K_marginal.mean() - 2 * K_marginal.mean(axis=1).mean()

    # Conditional MMD (per-class features)
    mmd_conditional = 0.0
    for c in np.unique(y_source):
        Xc_source = X_source[y_source == c]
        Xc_target = X_target[y_target == c]
        if len(Xc_target) == 0: continue
        K_cond = pairwise_kernels(Xc_source, Xc_target, metric='rbf', gamma=gamma)
        mmd_conditional += K_cond.mean() - 2 * K_cond.mean(axis=1).mean()

    # Dynamic alpha
    alpha = mmd_marginal / (mmd_marginal + mmd_conditional + 1e-8)
    return np.clip(alpha, 0.2, 0.8)  # Prevent extreme values

# Compute weights
w_marginal = compute_marginal_weights(X_train_scaled, X_val)
w_conditional = compute_conditional_weights(y_train, y_val.values.flatten())
alpha = compute_alpha(X_train_scaled, X_val, y_train, y_val.values.flatten())

# alphas = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]
alphas = [ 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]
for alpha in alphas:
    print(f"Alpha: {alpha}")
    # Combine weights dynamically
    final_weights = alpha * w_marginal + (1 - alpha) * w_conditional
    final_weights /= final_weights.mean()  # Stabilize

    # Train model
    dda_model = LogisticRegression(max_iter=1000, random_state=0, class_weight='balanced')
    dda_model.fit(X_train_scaled, y_train, sample_weight=final_weights)


    print("\nWith Covariate Adaptation(Dynamic Distribution Adaptation (DDA):")
    print("=== Test Set 2 (Labelled Validation) ===")
    dda_adapted_ce = evaluate_model(dda_model, X_val, y_val.values.flatten())

    print(f"Weighted CE Gained: {dda_adapted_ce - no_adpt_ce:.4f}")

ce_score_results['covariate_shift_adapted'].append(dda_adapted_ce)

"""### Approach 4: Pseudo-Labeling + Test-Time Adaptation
Concept: Use test set predictions to iteratively refine the model.
Goal: Leverages unlabeled test data via self-training.
"""

# Pseudo-label test data
test2_probs = base_model.predict_proba(X_test_two_unlabelled)
pseudo_labels = np.argmax(test2_probs, axis=1)

# Retrain with pseudo-labels
combined_X = np.vstack([X_train_scaled, X_test_two_unlabelled])
combined_y = np.concatenate([y_train, pseudo_labels])
pseudo_label_model = LogisticRegression(
                                max_iter=1000,
                                random_state=0,
                                class_weight='balanced')
pseudo_label_model.fit(combined_X, combined_y)

print("\nWith Covariate Adaptation(Pseudo-Labelling + Test-time Adaptation):")
print("=== Test Set 2 (Labelled Validation) ===")
pseudo_label_adapted_ce = evaluate_model(pseudo_label_model, X_val, y_val.values.flatten())

print(f"Weighted CE Gained: {pseudo_label_adapted_ce - no_adpt_ce:.4f}")

ce_score_results['covariate_shift_adapted'].append(pseudo_label_adapted_ce)

"""## Class-Conditional Covariate Shift
**Definition**:

Reference:
- Zhang et al. (2013) - "Domain Adaptation under Target and Conditional Shift
- Tachet des Combes et al. (2020) - "Domain Adaptation with Conditional Distribution Matching and Generalized Label Shift"

### Method 1: Class-Wise Kolmogorov-Smirnov (KS) Tests
**Concept** For each feature and class, compare distributions between training and test sets.  

**Goal**: Detect differences in feature distributions within the same class $P(X|Y))$.
Reference:
"""

from scipy.stats import ks_2samp

def detect_conditional_shift(X_source, X_target, y_source, y_target, feature_names):
    conditional_shift = {}
    classes = np.unique(y_source)

    print(f"Analyzing conditional shift across {len(classes)} classes")
    print(f"Source dataset: {X_source.shape[0]} samples, {X_source.shape[1]} features")
    print(f"Target dataset: {X_target.shape[0]} samples, {X_target.shape[1]} features")

    # Track metrics for summary
    total_shifts_by_class = {}
    most_shifted_features = {}

    for cls in classes:
        X_source_cls = X_source[y_source == cls]
        X_target_cls = X_target[y_target == cls]

        # Skip if class missing in target
        if len(X_target_cls) == 0:
            print(f"\nClass {cls}: Not present in target dataset - skipping")
            continue

        print(f"\nClass {cls}: Analyzing {len(X_source_cls)} source samples vs {len(X_target_cls)} target samples")

        class_shifts = []
        for i, feature in enumerate(feature_names):
            stat, p = ks_2samp(
                X_source_cls[:, i],
                X_target_cls[:, i]
            )

            if p < 0.05:  # Significant difference
                conditional_shift[(cls, feature)] = p
                class_shifts.append((feature, p))

        # Sort shifts by significance (lowest p-value first)
        class_shifts.sort(key=lambda x: x[1])

        # Store metrics
        total_shifts_by_class[cls] = len(class_shifts)

        # Print summary for this class
        if class_shifts:
            print(f"  Found {len(class_shifts)} shifted features for class {cls}")
            print(f"  Top 5 most shifted features for class {cls}:")
            for feat, p_val in class_shifts[:5]:
                # Calculate mean values to show direction of shift
                source_mean = np.mean(X_source_cls[:, feat])
                target_mean = np.mean(X_target_cls[:, feat])
                shift_direction = "higher" if target_mean > source_mean else "lower"
                shift_magnitude = abs((target_mean - source_mean) / source_mean) * 100 if source_mean != 0 else float('inf')

                print(f"    Feature {feat}: p-value = {p_val:.5f}, target mean is {shift_direction} by {shift_magnitude:.1f}% ({source_mean:.3f} → {target_mean:.3f})")

            # Store most shifted feature for this class
            if class_shifts:
                most_shifted_features[cls] = class_shifts[0][0]  # Feature with lowest p-value
        else:
            print(f"  No significant feature shifts detected for class {cls}")

    # Print overall summary
    print("\n--- OVERALL SHIFT SUMMARY ---")
    print(f"Total shifted features detected: {len(conditional_shift)}")

    if total_shifts_by_class:
        most_affected_class = max(total_shifts_by_class.items(), key=lambda x: x[1])[0]
        print(f"Most affected class: {most_affected_class} with {total_shifts_by_class[most_affected_class]} shifted features")

        # Print most shifted features by class
        print("\nMost significantly shifted feature by class:")
        for cls, feature in most_shifted_features.items():
            print(f"  Class {cls}: Feature {feature} (p-value = {conditional_shift.get((cls, feature), 'N/A'):.5f})")

    return conditional_shift

# Example usage:
shift_features = detect_conditional_shift(
    X_train_scaled, X_val,
    y_train, y_val.values.flatten(),
    feature_names=np.arange(300)
)
print(f"\nDetected {len(shift_features)} features with conditional shift")

"""### Approach 1: Class Conditional weights adapation"""

def class_conditional_weights(source_X, source_y, target_X, target_y):
    """
    Calculate class-conditional importance weights for domain adaptation.
    """
    # print(type(source_X))
    source_y_np = np.array(source_y).flatten()

    target_y_np = np.array(target_y).flatten()

    # Get unique classes
    unique_classes = np.unique(source_y_np)
    weights = np.ones(len(source_X))  # Initialize with ones

    for cls in unique_classes:
        # Get source/target indices for this class
        source_indices = np.where(source_y_np == cls)[0]
        target_indices = np.where(target_y_np == cls)[0]

        # Check if class is present in target
        if len(target_indices) == 0:
            # If class missing in target, set weight=0
            weights[source_indices] = 0
            continue

        # Extract source and target samples for this class

        source_X_cls = source_X[source_indices]

        target_X_cls = target_X[target_indices]

        # Compute class-specific domain weights
        X_domain = np.vstack([source_X_cls, target_X_cls])
        y_domain = np.concatenate([np.zeros(len(source_indices)), np.ones(len(target_indices))])

        try:
            lr = LogisticRegression(class_weight='balanced', max_iter=1000)
            lr.fit(X_domain, y_domain)

            # Get probabilities for source samples
            probs = lr.predict_proba(source_X_cls)

            # Add small constant to avoid division by zero
            epsilon = 1e-10
            cls_weights = (probs[:, 1] + epsilon) / (probs[:, 0] + epsilon)

            # Clip and normalize weights for stability
            cls_weights = np.clip(cls_weights, 0.1, 10)
            cls_weights = cls_weights / cls_weights.mean()

            # Assign weights using indices
            weights[source_indices] = cls_weights
        except Exception as e:
            print(f"Warning: Error computing weights for class {cls}: {e}")
            # Keep default weight of 1 for this class

    return weights


# compute weights
class_cond_weights = class_conditional_weights(
    source_X=X_train_scaled,
    source_y=y_train,
    target_X=X_val,
    target_y=y_val.values.flatten(),
)

 # Train model
cls_con_adpt_model = LogisticRegression(max_iter=1000, random_state=0, class_weight='balanced')
cls_con_adpt_model.fit(X_train_scaled, y_train, sample_weight=class_cond_weights)


print("\nWith Class Conditional Covariate Adaptation:")
print("=== Test Set 2 (Labelled Validation) ===")
cls_con_adapted_ce = evaluate_model(cls_con_adpt_model, X_val, y_val.values.flatten())

print(f"Weighted CE Gained: {cls_con_adapted_ce - no_adpt_ce:.4f}")
ce_score_results['class_conditional_shift_adapted'].append(cls_con_adapted_ce)

"""## Concept Distribution Shifting
Concept drift is when $P(Y|X)$ changes, but $P(X)$ remains the same.

### Method 1: Harmless concept shift
The Pred_proba has significant concept shift, but it is harmless. The model is still able to make accurate predictions on the test set, only the confidences vary.
> Ideas From: https://www.nannyml.com/blog/concept-drift#concept-drift-detection-with-nannyml
> Performance metrics such as accuracy, precision, recall, or f1-score will be the same for both models (ROC AUC will be impacted, though, since it uses the model scores rather than just class assignments).

> concept shift might occur in any region within the feature space. If it happens to be in a sparse region, its impact on the model's performance will be minor. This is because there is not much training nor serving data in this region. Thus, the model will hardly ever get to predict in this region. Any misclassifications caused by the concept shift in a sparse region will be rare events, not contributing much to the model’s overall performance.

Use F1-score as evaluation metric, compare the performance on train and test set use SVM (baseline) as the classifier.
"""

from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import f1_score, roc_auc_score
from sklearn.manifold import TSNE
import plotly.express as px

# split the labelled test data
X_test_two_label = X_test_two[:202]

# Compute mean feature vector for each class
class_means = []
unique_classes = np.unique(y_train)
for cls in unique_classes:
    class_means.append(X_train_df[y_train['label'] == cls].mean(axis=0))
class_means = np.array(class_means)  # Shape: (n_classes, n_features)

n_class_clusters = 4
agg_clustering = AgglomerativeClustering(n_clusters=4, linkage='ward')
class_clusters = agg_clustering.fit_predict(class_means)
class_to_cluster = {cls: cluster for cls, cluster in zip(unique_classes, class_clusters)}

train_f1_scores = []
test_f1_scores = []

for cluster_id in range(n_class_clusters):
    cluster_classes = [cls for cls in unique_classes if class_to_cluster[cls] == cluster_id]

    mask_train = np.isin(y_train, cluster_classes)
    X_cluster_train = X_train_df[mask_train]
    y_cluster_train = y_train[mask_train]

    model = SVC(C=5)
    model.fit(X_cluster_train, y_cluster_train)

    y_train_pred = model.predict(X_cluster_train)
    train_f1 = f1_score(y_cluster_train, y_train_pred, average='weighted')
    train_f1_scores.append(train_f1)

    mask_test = np.isin(y_test_two_label, cluster_classes)
    X_cluster_test = X_test_two_label[mask_test]
    y_cluster_test = y_test_two_label[mask_test]

    if len(y_cluster_test) == 0:
        test_f1 = 0
    else:
        y_test_pred = model.predict(X_cluster_test)
        test_f1 = f1_score(y_cluster_test, y_test_pred, average='weighted')
    test_f1_scores.append(test_f1)

clusters = range(n_class_clusters)

# plot the F1 scores for each cluster
plt.figure(figsize=(12, 6))
plt.bar(clusters, train_f1_scores, width=0.4, label='Train F1', align='center')
plt.bar(clusters, test_f1_scores, width=0.4, label='Test F1', align='edge', alpha=0.7)
plt.xticks(clusters, labels=[f'Cluster {i}' for i in clusters])
plt.xlabel('Cluster')
plt.ylabel('F1 Score')
plt.title('F1 Score Comparison between Train and Test per Cluster')
plt.legend()
plt.show()

# plot the f1 score difference between train and test per cluster
f1_diff = np.array(train_f1_scores) - np.array(test_f1_scores)
plt.figure(figsize=(12, 6))
plt.bar(clusters, f1_diff, color='red', alpha=0.6)
plt.xticks(clusters, labels=[f'Cluster {i}' for i in clusters])
plt.xlabel('Cluster')
plt.ylabel('F1 Difference (Train - Test)')
plt.title('Concept Drift Indicator: F1 Score Differences')
plt.axhline(0, color='black', linestyle='--')
plt.show()

"""## Results:

"""

print(ce_score_results)
# {'base_model': [0.03484008623290702],
#  'prior_shift_adapted': [0.029963420426361676],
#  'covariate_shift_adapted': [0.023665850186936337, 0.06135232981543142, 0.024496340715960633, 0.02438872950123746],  # kernel-based, CORAL, DDA, Pseudo-labelling,
#  'class_conditional_shift_adapted': [0.020847344469894735]}

import matplotlib.pyplot as plt

# Method labels for covariate shift techniques
covariate_methods = ['Kernel-based', 'CORAL', 'DDA', 'Pseudo-labelling']

# Prepare data for plotting
categories = list(ce_score_results.keys())
values = [v[0] if len(v) == 1 else v for v in ce_score_results.values()]

# Create a figure with two subplots
fig, ax1 = plt.subplots(1, 1, figsize=(10, 5))

# --------------------------
# Grouped Bar Plot
# --------------------------
x = np.arange(len(categories))
width = 0.2

# Base model and single-value methods
ax1.bar(x[0], values[0], width, label='Base Model')
ax1.bar(x[1], values[1], width, label='Prior Shift Adapted')
ax1.bar(x[3], values[3], width, label='Class Conditional Adapted')

# Covariate shift methods
for i, (val, label) in enumerate(zip(values[2], covariate_methods)):
    ax1.bar(x[2] + width*i, val, width, label=f'Covariate: {label}')

ax1.set_ylabel('CE Score', fontsize=12)
ax1.set_title('Model Performance Comparison by Adaptation Type', fontsize=14)
ax1.set_xticks(x)
ax1.set_xticklabels(categories, rotation=45, ha='right')
ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
ax1.grid(True, linestyle='--', alpha=0.6)

# --------------------------
# Method-wise Comparison Plot
# --------------------------
all_methods = [
    'Base Model',
    'Prior Shift Adapted',
    *[f'Covariate: {m}' for m in covariate_methods],
    'Class Conditional Adapted'
]
# all_scores = [
#     values[0][0],
#     values[1][0],
#     *values[2],
#     values[3][0]
# ]

# # Sort methods by score
# sorted_indices = np.argsort(all_scores)
# sorted_methods = [all_methods[i] for i in sorted_indices]
# sorted_scores = [all_scores[i] for i in sorted_indices]

# colors = plt.cm.viridis(np.linspace(0, 1, len(all_methods)))
# ax2.barh(sorted_methods, sorted_scores, color=colors)
# ax2.set_xlabel('CE Score (Lower is Better)', fontsize=12)
# ax2.set_title('Method Performance Ranking', fontsize=14)
# ax2.grid(True, linestyle='--', alpha=0.6)

# # Add value labels
# for i, v in enumerate(sorted_scores):
#     ax2.text(v + 0.001, i, f'{v:.4f}', color='black', va='center')

# plt.tight_layout()
# plt.show()

"""## Visualisation"""

# Visualization with t-SNE
from sklearn.manifold import TSNE
from umap import UMAP
import seaborn as sns
import matplotlib.pyplot as plt

def visualize_domain_shift(dataset1, dataset2, title=None,
                          method='tsne', perplexity=30,
                          with_contours=False, figsize=(12,8)):
    """
    Visualize domain shift between two datasets using dimensionality reduction.

    Parameters:
    - dataset1: First dataset (e.g., training data)
    - dataset2: Second dataset (e.g., test data)
    - title: Plot title
    - method: 'tsne' or 'umap'
    - perplexity: t-SNE perplexity (ignored for UMAP)
    - with_contours: Add density contours
    """
    # Combine datasets and create labels
    combined = np.vstack([dataset1, dataset2])
    labels = np.hstack([np.zeros(len(dataset1)), np.ones(len(dataset2))])

    # Dimensionality reduction
    if method.lower() == 'umap':
        proj = UMAP(random_state=42).fit_transform(combined)
    else:
        proj = TSNE(n_components=2, perplexity=perplexity,
                   random_state=42).fit_transform(combined)

    # Split projections
    proj1 = proj[labels == 0]
    proj2 = proj[labels == 1]

    # Create plot
    plt.figure(figsize=figsize)

    # Plot with different styles for each dataset
    scatter1 = plt.scatter(proj1[:, 0], proj1[:, 1],
                          c='#a6cee3', alpha=0.8, s=40,
                          edgecolor='w', linewidth=0.3,
                          label='Dataset 1')
    scatter2 = plt.scatter(proj2[:, 0], proj2[:, 1],
                          c='#33a02c', alpha=0.8, s=40, marker='X',
                          edgecolor='w', linewidth=0.2,
                          label='Dataset 2')

    # Add density contours if requested
    if with_contours:
        sns.kdeplot(x=proj1[:, 0], y=proj1[:, 1],
                   cmap='#a6cee3', alpha=0.4, levels=5)
        sns.kdeplot(x=proj2[:, 0], y=proj2[:, 1],
                   cmap='#33a02c', alpha=0.4, levels=5)

    plt.title(f'Domain Shift Visualization: {title}' if title else 'Domain Shift Visualization')
    plt.xlabel(f'{method.upper()} Dimension 1')
    plt.ylabel(f'{method.upper()} Dimension 2')
    plt.legend()
    plt.grid(alpha=0.2)
    plt.savefig(f"plots/results/{title}.png")
    plt.show()

visualize_domain_shift(X_train_df, X_train_df, 'Self comparison')

visualize_domain_shift(X_train_df, X_test_one, 'Training - Test One')

visualize_domain_shift(X_train_df, X_test_two, 'Training - Test Two(Whole)')

visualize_domain_shift(X_train_df, X_test_two_labelled, 'Training - Test Two(labelled)')

visualize_domain_shift(X_train_df, X_test_two_unlabelled, 'Training - Test Two(unlabelled)')

visualize_domain_shift(X_test_two_labelled, X_test_two_unlabelled, 'Test Two(labelled) - Test Two(unlabelled)')

# def weighted_log_loss(y_true, y_pred):
#     """
#     Compute the weighted cross-entropy (log loss) given true labels and predicted probabilities.
#     Parameters:
#     - y_true: (N, C) One-hot encoded true labels
#     - y_pred: (N, C) Predicted probabilities
#     Returns:
#     - Weighted log loss (scalar).
#     """

#     print(y_true.shape)  # (202, 28)
#     print(y_pred.shape)  # (202, 28)

#     # Compute class frequencies
#     class_counts = np.sum(y_true, axis=0) # Sum over samples to get counts per class   # (28,)  # first five elements [0 0 1 1 4]
#     class_weights = 1.0 / class_counts   # some values in the class counts is zero
#     class_weights /= np.sum(class_weights) # Normalize weights to sum to 1
#     # Compute weighted loss
#     sample_weights = np.sum(y_true * class_weights, axis=1) # Get weight for each sample

#     loss = -np.mean(sample_weights * np.sum(y_true * np.log(y_pred), axis=1))
#     return loss


# # Convert discrete labels to one-hot encoded format
# def to_one_hot(labels, num_classes):
#     one_hot = np.zeros((len(labels), num_classes), dtype=np.int32)
#     for i, label in enumerate(labels):
#         if 0 <= label < num_classes:  # Safety check
#             one_hot[i, label] = 1
#     return one_hot

# # Convert true labels to one-hot format
# y_true_one_hot = to_one_hot(y_val['label'], 28)  # (202, 28)


# y_test_1_ohe = (np.arange(28) == np.random.choice(28, size=202)[:, None]).astype(int)  # (202, 28) example row: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]

# pred_label = base_model.predict(X_val)
# preds_1 = base_model.predict_proba(X_val)  # (202, 28)
# # loss_1 = weighted_log_loss(y_true_one_hot , preds_1) # preds_1 should be (202, 28)
# # print(loss_1)


# loss_test = weighted_log_loss(y_val, preds_1)  # weighted ce score: 0.020650833987178645
# # previous result: Weighted CE score: 0.03484008623290702
# print(loss_test)
# loss_original = weighted_log_loss_test(y_val, pred_label)
# print(loss_original)



