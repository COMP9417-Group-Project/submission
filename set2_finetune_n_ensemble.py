# -*- coding: utf-8 -*-
"""set2_finetune_n_ensemble (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TwL8_YBJugjd20KHpC9B7jJXwBoePw2z
"""

# import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import numpy as np
import os
from collections import Counter
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
from sklearn.utils.class_weight import compute_class_weight
import logging
from imblearn.over_sampling import SMOTE
from sklearn.feature_selection import RFE
import random

import sys

# define path
y_train_path = 'y_train.csv'
y_test_path = 'y_test_2_reduced.csv'
x_train_path = 'X_train.csv'
x_test_one_path = 'X_test_1.csv'
x_test_two_path = 'X_test_2.csv'


# read df
y_train = pd.read_csv(y_train_path)
y_test_two_label = pd.read_csv(y_test_path)
X_train_df = pd.read_csv(x_train_path)
X_test_one = pd.read_csv(x_test_one_path)
X_test_two = pd.read_csv(x_test_two_path)

test_length = y_test_two_label.shape[0]
test_indices = np.arange(test_length)
X_test_two_labelled = X_test_two.iloc[test_indices]
mask = np.ones(len(X_test_two), dtype=bool)
mask[test_indices] = False
X_test_two_unlabelled = X_test_two.iloc[mask]

# print(X_test_two_labelled.shape)
# print(X_test_two_unlabelled.shape)



# define scaler
scaler = MinMaxScaler()
scaler.fit(X_train_df)

# transform data
X_train_scaled = scaler.transform(X_train_df)
X_test_scaled_labelled = scaler.transform(X_test_two_labelled)
X_test_scaled_unlabelled = scaler.transform(X_test_two_unlabelled)


# # based on the y label frequency, sort the the label
# labels = y_train.values.flatten()
# label_frequency = Counter(labels)
# sorted_labels = [label for label, _ in label_frequency.most_common()]
# # print(label_frequency)
# sorted_labels.reverse()
# num_classes = len(sorted_labels)


# weighted log loss score calculation method
def weighted_log_loss(y_true_np, y_pred):
    """
    Compute the weighted cross-entropy (log loss) given true labels and predicted probabilities.

    Parameters:
    - y_true: 1-d array like object contains (range 0-27), shape: (n_samples,)
    - y_pred: array like object contains list of probabilities, shape: (n_samples, 28)

    Returns:
    - Weighted log loss (scalar).
    """
    import numpy as np
    import pandas as pd

    # Number of classes
    n_classes = 28  # Classes 0-27

    # Convert discrete labels to one-hot encoded format
    def to_one_hot(labels, num_classes):
        one_hot = np.zeros((len(labels), num_classes))
        for i, label in enumerate(labels):
            if 0 <= label < num_classes:  # Safety check
                one_hot[i, label] = 1
        return one_hot

    # Convert true labels to one-hot format
    y_true_one_hot = to_one_hot(y_true_np, n_classes)


    # Compute class frequencies
    class_counts = np.sum(y_true_one_hot, axis=0)  # Sum over samples to get counts per class

    # Compute class weights with safety check for zero counts
    class_weights = np.zeros_like(class_counts)
    for c in range(n_classes):
        if class_counts[c] > 0:  # Avoid division by zero
            class_weights[c] = 1.0 / class_counts[c]

    # Normalize weights to sum to 1
    class_weights /= np.sum(class_weights)

    # Compute weighted loss
    sample_weights = np.sum(y_true_one_hot * class_weights, axis=1)  # Get weight for each sample

    # Calculate log loss term
    log_terms = np.sum(y_true_one_hot * np.log(y_pred), axis=1)
    loss = -np.mean(sample_weights * log_terms)

    return loss

def evaluate_model(model, X, y_true):
    """
        model: fitted model
        X: feature matrix (n_sample, 300)
        y_true: array, contains true labels (0-27), (n_samples, )
    """
    y_pred_prob = model.predict_proba(X)  # (n_samples, 28)
    y_pred_label = model.predict(X)
    ce = weighted_log_loss(y_true, y_pred_prob)
    print(f"Weighted CE: {ce:.5f}")
    print(classification_report(y_true, y_pred_label))
    return ce

"""### Compare the effect of adaptive weights on the same model"""

def class_conditional_weights(source_X, source_y, target_X, target_y):
    """
    Calculate class-conditional importance weights for domain adaptation.
    """
    print(type(source_X))
    source_y_np = np.array(source_y).flatten()
    target_y_np = np.array(target_y).flatten()

    # Get unique classes
    unique_classes = np.unique(source_y_np)
    weights = np.ones(len(source_X))  # Initialize with ones

    for cls in unique_classes:
        # Get source/target indices for this class
        source_indices = np.where(source_y_np == cls)[0]
        target_indices = np.where(target_y_np == cls)[0]

        # Check if class is present in target
        if len(target_indices) == 0:
            # If class missing in target, set weight=0
            weights[source_indices] = 0
            continue

        # Extract source and target samples for this class

        source_X_cls = source_X[source_indices]

        target_X_cls = target_X[target_indices]

        # Compute class-specific domain weights
        X_domain = np.vstack([source_X_cls, target_X_cls])
        y_domain = np.concatenate([np.zeros(len(source_indices)), np.ones(len(target_indices))])

        try:
            lr = LogisticRegression(class_weight='balanced', max_iter=1000)
            lr.fit(X_domain, y_domain)

            # Get probabilities for source samples
            probs = lr.predict_proba(source_X_cls)

            # Add small constant to avoid division by zero
            epsilon = 1e-10
            cls_weights = (probs[:, 1] + epsilon) / (probs[:, 0] + epsilon)

            # Clip and normalize weights for stability
            cls_weights = np.clip(cls_weights, 0.1, 10)
            cls_weights = cls_weights / cls_weights.mean()

            # Assign weights using indices
            weights[source_indices] = cls_weights
        except Exception as e:
            print(f"Warning: Error computing weights for class {cls}: {e}")
            # Keep default weight of 1 for this class

    return weights



# without weights adaptive
# fit models
base_model = LogisticRegression(max_iter=2000, random_state=0)
base_model.fit(X_train_scaled, y_train)



# evaluate base model
print(f"="*50)
print(f"Resutls without weights adaptation")
base_lg_ce = evaluate_model(base_model, X_test_scaled_labelled, y_test_two_label.values.flatten())
base_lg_pred = base_model.predict(X_test_scaled_labelled)


# # adaptive weights model
# # get class conditional weights
# # For each class, compute domain weights separately
weights = class_conditional_weights(
    source_X=X_train_scaled,
    source_y=y_train.values.flatten(),
    target_X=X_test_scaled_labelled,
    target_y=y_test_two_label.values.flatten(),
)

adpative_clf = LogisticRegression(random_state=0, max_iter=2000)
adpative_clf.fit(X_train_scaled, y_train, sample_weight=weights)


# evaluate adapative model
print(f"="*50)
print(f"Resutls with weights adaptation")
adaptive_lg_ce = evaluate_model(adpative_clf, X_test_scaled_labelled, y_test_two_label.values.flatten())
adaptive_lg_pred = adpative_clf.predict(X_test_scaled_labelled)

print(f"Weigthed CE gained: {base_lg_ce - adaptive_lg_ce:.4f}")

##
# Add new imports at the top
from sklearn.calibration import calibration_curve
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import seaborn as sns

def compare_class_performance(y_true, pred1, pred2):
    """Compare per-class F1 scores between two models"""
    # Get complete list of unique classes present in any report
    report1 = classification_report(y_true, pred1, output_dict=True, zero_division=0)
    report2 = classification_report(y_true, pred2, output_dict=True, zero_division=0)

    # Get all unique classes from both reports
    all_classes = set(report1.keys()).union(report2.keys())
    class_names = [str(k) for k in all_classes
                  if k not in ('accuracy', 'macro avg', 'weighted avg')]

    # Sort classes numerically if possible
    try:
        class_names = sorted(class_names, key=lambda x: int(x))
    except ValueError:
        class_names = sorted(class_names)

    f1_diff = []
    for cls in class_names:
        # Handle missing classes in either report
        f1_1 = report1.get(cls, {}).get('f1-score', 0)
        f1_2 = report2.get(cls, {}).get('f1-score', 0)
        f1_diff.append(f1_2 - f1_1)

    plt.figure(figsize=(15, 6))
    bars = plt.bar(class_names, f1_diff)

    # Add value labels
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                 f'{height:.2f}', ha='center', va='bottom')

    plt.axhline(0, color='black', linestyle='--')
    plt.title('F1 Score Improvement by Class (Adaptive Logistic Regression vs Baseline Logistic Regression)')
    plt.ylabel('F1 Score Difference')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(f"plots/results/Class_conditional_weight_adpted_vs_baseline.png")
    plt.show()

compare_class_performance(y_test_two_label, base_lg_pred, adaptive_lg_pred)

"""### Discussion about weights adaptation
Based on the above result, the weight adaptation method improved performance of the model on the most of the classes that effectively migtigate the effect of the conditional shift on the
"""

# Compare with baseline model
print(f"="*50)
print(f"Fine tuned lg adapted model results")
trial_clf = LogisticRegression(random_state=0, class_weight='balanced', max_iter=1000, C=0.7, penalty='l2')
trial_clf.fit(X_train_scaled, y_train, sample_weight=weights)

# evaluation
trial_lg_ce = evaluate_model(trial_clf, X_test_scaled_labelled, y_test_two_label.values.flatten())
trial_pred = trial_clf.predict(X_test_scaled_labelled)

print(f"Weigthed CE gained: {base_lg_ce - trial_lg_ce:.5f}")

# best ce logistic regression model
# trial_clf = LogisticRegression(random_state=0, class_weight='balanced', max_iter=1000, C=0.7, penalty='l2')
# Weighted CE: 0.0123
#               precision    recall  f1-score   support

#            2       0.00      0.00      0.00         1
#            3       0.00      0.00      0.00         1
#            4       0.57      1.00      0.73         4
#            5       1.00      0.89      0.94         9
#            6       0.75      1.00      0.86         3
#            7       0.91      0.71      0.80        14
#            8       0.86      0.83      0.84        29
#            9       0.50      0.33      0.40         3
#           10       0.67      1.00      0.80         2
#           11       0.90      1.00      0.95         9
#           12       0.83      0.67      0.74        43
#           13       0.11      1.00      0.20         1
#           14       1.00      1.00      1.00         6
#           15       1.00      1.00      1.00         1
#           17       0.90      0.90      0.90        10
#           18       0.00      0.00      0.00         1
#           19       0.50      0.50      0.50         4
#           20       0.67      0.67      0.67         3
#           21       1.00      1.00      1.00         7
#           23       0.25      1.00      0.40         1
#           24       0.59      0.67      0.62        15
#           25       0.90      0.69      0.78        26
#           26       0.00      0.00      0.00         1
#           27       1.00      0.88      0.93         8

#     accuracy                           0.76       202
#    macro avg       0.62      0.70      0.63       202
# weighted avg       0.82      0.76      0.78       202

# Weigthed CE gained: 0.0083


# best macro avg F1 logistic regression model

# trial_clf = LogisticRegression(random_state=0, class_weight='balanced', C=32, penalty='l2')
# Weighted CE: 0.0137
#               precision    recall  f1-score   support

#            2       0.00      0.00      0.00         1
#            3       0.00      0.00      0.00         1
#            4       0.44      1.00      0.62         4
#            5       1.00      1.00      1.00         9
#            6       0.60      1.00      0.75         3
#            7       0.83      0.71      0.77        14
#            8       0.88      0.79      0.84        29
#            9       1.00      0.33      0.50         3
#           10       0.67      1.00      0.80         2
#           11       1.00      1.00      1.00         9
#           12       0.86      0.72      0.78        43
#           13       0.14      1.00      0.25         1
#           14       1.00      1.00      1.00         6
#           15       1.00      1.00      1.00         1
#           17       0.90      0.90      0.90        10
#           18       0.00      0.00      0.00         1
#           19       0.60      0.75      0.67         4
#           20       0.67      0.67      0.67         3
#           21       1.00      1.00      1.00         7
#           23       0.33      1.00      0.50         1
#           24       0.59      0.67      0.62        15
#           25       0.89      0.65      0.76        26
#           26       0.20      1.00      0.33         1
#           27       1.00      0.88      0.93         8

#     accuracy                           0.78       202
#    macro avg       0.65      0.75      0.65       202
# weighted avg       0.83      0.78      0.79       202

# Weigthed CE gained: 0.0069

"""### Trials on other models and fine tuning"""

from sklearn.svm import SVC
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.utils import compute_sample_weight
from sklearn.ensemble import HistGradientBoostingClassifier


# define a simple svm model
simple_sgd = SGDClassifier(random_state=0, loss='log_loss')
simple_sgd.fit(X_train_scaled, y_train)
base_sgd_ce = evaluate_model(simple_sgd, X_test_scaled_labelled, y_test_two_label.values.flatten())


# # abdaptive svm
adpative_sgd = SGDClassifier(random_state=0, loss='log_loss', tol=1e-3 )
adpative_sgd.fit(X_train_scaled, y_train, sample_weight=weights)


# evaluate adapative model
print(f"="*50)
print(f"Resutls with weights adaptation")
adaptive_sgd_ce = evaluate_model(adpative_sgd, X_test_scaled_labelled, y_test_two_label.values.flatten())
adaptive_sgd_pred = adpative_sgd.predict(X_test_scaled_labelled)

print(f"Weigthed CE gained: {base_sgd_ce - adaptive_sgd_ce:.4f}")

"""### Ensemble on Adaptive Logistic Regression and SGD"""

import numpy as np
from sklearn.base import BaseEstimator, ClassifierMixin

class ClassSpecificEnsemble(BaseEstimator, ClassifierMixin):
    def __init__(self, sgd_model, lr_model, sgd_classes):
        self.sgd_model = sgd_model  # Must be fitted before ensemble.fit()
        self.lr_model = lr_model    # Must be fitted before ensemble.fit()
        self.sgd_classes = sgd_classes  # e.g., [4, 13, 26]

    def fit(self, X, y):
        # Verify models are already fitted and classes match
        if not hasattr(self.lr_model, 'classes_'):
            raise ValueError("LogisticRegression model must be fitted first.")
        if not hasattr(self.sgd_model, 'classes_'):
            raise ValueError("SGD model must be fitted first.")
        if not np.array_equal(self.lr_model.classes_, self.sgd_model.classes_):
            raise ValueError("SGD and LogisticRegression have different class labels.")
        self.classes_ = self.lr_model.classes_  # Set after validation
        return self

    def predict_proba(self, X):
        sgd_proba = self.sgd_model.predict_proba(X)
        lr_proba = self.lr_model.predict_proba(X)
        combined_proba = lr_proba.copy()
        combined_proba[:, self.sgd_classes] = sgd_proba[:, self.sgd_classes]
        combined_proba /= combined_proba.sum(axis=1, keepdims=True)  # Renormalize
        return combined_proba

    def predict(self, X):
        return np.argmax(self.predict_proba(X), axis=1)

# Define classes to prioritize with SGD
sgd_target_classes = [4, 13, 26]

# Train models (already done in your code)
best_lg = LogisticRegression(random_state=0, class_weight='balanced', max_iter=1000, C=0.7, penalty='l2')
best_lg.fit(X_train_scaled, y_train, sample_weight=weights)
best_sgd = SGDClassifier(random_state=0, loss='log_loss', tol=1e-3 )
best_sgd.fit(X_train_scaled, y_train, sample_weight=weights)


# Create ensemble
ensemble = ClassSpecificEnsemble(
    sgd_model=best_sgd,
    lr_model=best_lg,
    sgd_classes=sgd_target_classes
)
ensemble.fit(X_train_scaled, y_train)
ensemble_ce = evaluate_model(ensemble, X_test_scaled_labelled, y_test_two_label.values.flatten())

"""### Trials of feature selection on best performed adpative model"""

import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectFromModel

# Assuming X_train_scaled and X_val_scaled are properly scaled versions
# First identify redundant features using correlation analysis

# Convert scaled data to DataFrame
X_train_df = pd.DataFrame(X_train_scaled)

# Calculate correlation matrix
corr_matrix = X_train_df.corr().abs()

# Select upper triangle of correlation matrix
upper = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
upper_tri = corr_matrix.where(upper)

# Find features with correlation above threshold (0.9)
to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]

# Get column indices to drop
to_drop_indices = [int(col) for col in to_drop]

# Remove redundant features from both training and validation sets
X_train_reduced = np.delete(X_train_scaled, to_drop_indices, axis=1)
X_test_reduced = np.delete(X_test_scaled_labelled, to_drop_indices, axis=1)  # Ensure X_val is scaled first!


# Now train and evaluate with reduced features
print(f"="*50)
print(f"Reduced feature model results")
feature_reduced_clf = LogisticRegression(random_state=0, class_weight='balanced', max_iter=1000, C=0.7, penalty='l2')
feature_reduced_clf.fit(X_train_reduced, y_train, sample_weight=weights)
trial_pred = feature_reduced_clf.predict(X_test_reduced)
feature_reduced_lg_ce = evaluate_model(feature_reduced_clf, X_test_reduced, y_test_two_label.values.flatten())
print(f"Weigthed CE gained: {base_lg_ce - feature_reduced_lg_ce:.4f}")

"""### Discussion:
The reduced features using feature correlation to remove rundant feature does not improve either macro-F1 score and weighted ce loss.

### Pseudo Labelling Test 2 and help predict test 1
"""

scores = {
    'baseline':[],
    'combined_train_test2_true_labelled': [],
    'combined_train_test2_pseudo_labelled':[],
    'combined_train_test2_pseudo_labelled(filtered)': [],
    'combined_train_test2_pseudo_n_true_labelled': []
}

# Organize into logical sections
# ============================== Data Preparation ==============================
# Split train/val
X_train_set1, X_val_set1, y_train_set1, y_val_set1 = train_test_split(
    X_train_df, y_train, test_size=0.2, random_state=0, stratify=y_train)

# Scale data
scaler = MinMaxScaler().fit(X_train_set1)
X_train_set1_scaled = scaler.transform(X_train_set1)
X_val_set1_scaled = scaler.transform(X_val_set1)
X_set2_scaled_unlabelled = scaler.transform(X_test_two_unlabelled)
X_set2_scaled_labelled = scaler.transform(X_test_two_labelled)

# ============================== Model Evaluation Function =====================
def train_and_evaluate_model(X_train, y_train, X_val, y_val, model):
    model.fit(X_train, y_train)
    return evaluate_model(model, X_val, y_val)

# ============================== Baseline Model =================================
set1_baseline = LogisticRegression(max_iter=2000, random_state=0)
scores['baseline'] = train_and_evaluate_model(
    X_train_set1_scaled, y_train_set1.values.flatten(),
    X_val_set1_scaled, y_val_set1.values.flatten(),
    set1_baseline
)

# ============================== Pseudo-label Strategies ========================
def get_filtered_pseudo(X, threshold=0.5):
    proba = ensemble.predict_proba(X)
    confidence = np.max(proba, axis=1)
    mask = confidence >= threshold
    return X[mask], pseudo_set2_labels[mask]

# Strategy 1: Combined with true labelled test2
X_comb_true = np.concatenate([X_train_set1_scaled, X_set2_scaled_labelled], axis=0)
y_comb_true = np.concatenate([y_train_set1.values.flatten(), y_test_two_label.values.flatten()], axis=0)
scores['combined_train_test2_true_labelled'] = train_and_evaluate_model(
    X_comb_true, y_comb_true, X_val_set1_scaled, y_val_set1.values.flatten(),
    LogisticRegression(max_iter=2000, random_state=0)
)

# Strategy 2: Pseudo-labelled (unfiltered)
X_comb_pseudo = np.concatenate([X_train_set1_scaled, X_set2_scaled_unlabelled], axis=0)
y_comb_pseudo = np.concatenate([y_train_set1.values.flatten(), pseudo_set2_labels], axis=0)
scores['combined_train_test2_pseudo_labelled'] = train_and_evaluate_model(
    X_comb_pseudo, y_comb_pseudo, X_val_set1_scaled, y_val_set1.values.flatten(),
    LogisticRegression(max_iter=2000, random_state=0)
)

# Strategy 3: Pseudo-labelled (filtered)
X_pseudo_filt, y_pseudo_filt = get_filtered_pseudo(X_set2_scaled_unlabelled)
X_comb_filt = np.concatenate([X_train_set1_scaled, X_pseudo_filt], axis=0)
y_comb_filt = np.concatenate([y_train_set1.values.flatten(), y_pseudo_filt], axis=0)
scores['combined_train_test2_pseudo_labelled(filtered)'] = train_and_evaluate_model(
    X_comb_filt, y_comb_filt, X_val_set1_scaled, y_val_set1.values.flatten(),
    LogisticRegression(max_iter=2000, random_state=0)
)

# Strategy 4: Combined pseudo + true labels
X_comb_all = np.concatenate([
    X_train_set1_scaled,
    X_set2_scaled_unlabelled,
    X_set2_scaled_labelled
], axis=0)
y_comb_all = np.concatenate([
    y_train_set1.values.flatten(),
    pseudo_set2_labels,
    y_test_two_label.values.flatten()
], axis=0)
scores['combined_train_test2_pseudo_n_true_labelled'] = train_and_evaluate_model(
    X_comb_all, y_comb_all, X_val_set1_scaled, y_val_set1.values.flatten(),
    LogisticRegression(max_iter=2000, random_state=0)
)

# ============================== Comparison Plot ================================
plt.figure(figsize=(12, 6))
methods = list(scores.keys())
values = [scores[m] for m in methods]

bars = plt.bar(methods, values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])
plt.ylabel('weighted CE Loss', fontsize=12)
plt.title('Model Comparison: Multi-source Training Data Integration', fontsize=12)
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.ylim(min(values)*0.95, max(values)*1.05)

# Add value labels
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
             f'{height:.5f}',
             ha='center', va='bottom')

plt.tight_layout()
plt.savefig('plots/results/training_data_integration.png')
plt.show()

