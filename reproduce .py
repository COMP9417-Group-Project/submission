# -*- coding: utf-8 -*-
"""reproduce (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g8TSIfXqiiZ1hSH0VrWLPebztvr-Hntc
"""

import numpy as np
import pandas as pd

from sklearn.metrics import classification_report

def weighted_log_loss_test(y_true_np, y_pred):
    """
    Compute the weighted cross-entropy (log loss) given true labels and predicted probabilities.

    Parameters:
    - y_true: 1-d array like object contains (range 0-27), shape: (n_samples,)
    - y_pred: array like object contains list of probabilities, shape: (n_samples, 28)

    Returns:
    - Weighted log loss (scalar).
    """

    # Number of classes
    n_classes = 28  # Classes 0-27

    # Convert discrete labels to one-hot encoded format
    def to_one_hot(labels, num_classes):
        one_hot = np.zeros((len(labels), num_classes))
        for i, label in enumerate(labels):
            if 0 <= label < num_classes:  # Safety check
                one_hot[i, label] = 1
        return one_hot

    # Convert true labels to one-hot format
    y_true_one_hot = to_one_hot(y_true_np, n_classes)


    # Compute class frequencies
    class_counts = np.sum(y_true_one_hot, axis=0)  # Sum over samples to get counts per class

    # Compute class weights with safety check for zero counts
    class_weights = np.zeros_like(class_counts)
    for c in range(n_classes):
        if class_counts[c] > 0:  # Avoid division by zero
            class_weights[c] = 1.0 / class_counts[c]

    # Normalize weights to sum to 1
    class_weights /= np.sum(class_weights)

    # Compute weighted loss
    sample_weights = np.sum(y_true_one_hot * class_weights, axis=1)  # Get weight for each sample

    y_pred = np.clip(y_pred, 1e-15, 1.0 - 1e-15)

    # Calculate log loss term
    log_terms = np.sum(y_true_one_hot * np.log(y_pred), axis=1)
    loss = -np.mean(sample_weights * log_terms)

    return loss


def evaluate_model(model, X, y_true):
    """
        model: fitted model
        X: feature matrix (n_sample, 300)
        y_true: array, contains true labels (0-27), (n_samples, )
    """
    y_pred_prob = model.predict_proba(X)  # (n_samples, 28)
    y_pred_label = model.predict(X)
    ce = weighted_log_loss_test(y_true, y_pred_prob)
    print(f"Weighted CE: {ce:.4f}")
    print(classification_report(y_true, y_pred_label))
    cr = classification_report(y_true, y_pred_label, output_dict=True)
    wf1 = cr['weighted avg']['f1-score']
    return ce, wf1

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import make_scorer
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

# Load data
X_train = pd.read_csv('../../X_train.csv').values
y_train = pd.read_csv('../../y_train.csv').values.ravel()
X_test = pd.read_csv('../../X_test_2.csv').head(202)
y_test = pd.read_csv('../../y_test_2_reduced.csv').values.ravel()

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)

# model = LogisticRegression(random_state=0, max_iter=1000)

model = SVC(C=5, probability=True, random_state=0)

# model = RandomForestClassifier(
#         n_estimators=500,
#         max_depth=10,
#         min_samples_split=10,
#         min_samples_leaf=1,
#         max_features='log2',
#         criterion='entropy',
#         n_jobs=-1,
#         random_state=42
#     )

# model = CatBoostClassifier(
#         iterations=2000,
#         task_type='GPU',
#         random_state=9417,
#         learning_rate=0.03,
#         loss_function='MultiClass',
#         eval_metric='Accuracy',
#         )

params = {
    'objective': 'mutli:softprob',
    'eval_metric': 'logloss',
    'max_delta_step': 1,
    'subsample': 0.8,
    'colsample_bytree': 0.7,
    'tree_method': 'hist',
    'alpha': 0.5,
    'lambda': 1.5,
}


folds_ce = []
folds_wf1 = []

for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X_train, y_train)):
    print(f"Fold {fold_idx+1}")
    X_train_fold, X_test_fold = X_train[train_idx], X_train[test_idx]
    y_train_fold, y_test_fold = y_train[train_idx], y_train[test_idx]

    scaler = MinMaxScaler()
    X_train_fold = scaler.fit_transform(X_train_fold)
    X_test_fold = scaler.transform(X_test_fold)
    print("Training...")
    model.fit(X_train_fold, y_train_fold)

    # dtrain_fold = xgb.DMatrix(X_train_fold, label=y_train_fold)
    # dval_fold = xgb.DMatrix(X_test_fold, label=y_test_fold)
    # model = xgb.train(
    #     params,
    #     dtrain_fold,
    #     num_boost_round=1000,
    #     evals=[(dval_fold, 'validation')],
    #     early_stopping_rounds=50,
    #     verbose_eval=100
    # )

    print("Valiadating...")
    #y_pred_prob = model.predict(dval_fold)
    ce, wf1= evaluate_model(model, X_test_fold, y_test_fold)
    # y_pred_prob = model.predict(X_test_fold)  # (n_samples, 28)
    # y_pred_label = (y_pred_prob > 0.5).astype(int)
    # ce = weighted_log_loss_test(y_test_fold, y_pred_prob)
    # print(f"Weighted CE: {ce:.4f}")
    # print(classification_report(y_test_fold, y_pred_label))
    # cr = classification_report(y_test_fold, y_pred_label, output_dict=True)
    # wf1 = cr['weighted avg']['f1-score']

    folds_ce.append(ce)
    folds_wf1.append(wf1)
    print("="*50)
print(f"Average weighted cross-entropy loss across all folds: {np.array(folds_ce).mean():.8f}")
print(f"Standard deviation of the loss across all folds: {np.array(folds_ce).std():.8f}")
print(f"Weighted F1 score across all folds: {np.array(folds_wf1).mean():.8f}")
print(f"Standard deviation of the F1 score across all folds: {np.array(folds_wf1).std():.8f}")